{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad36d88-f3ee-483e-b6bd-d5f21065b683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import quic_graph_lasso\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988204a1-d822-4c7b-9d6e-57fc04521dcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preliminary tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dcca0d-aa9f-4d1e-bf47-ac680a24d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(0,100, size=(5,5))\n",
    "cov = np.cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9319a814-8428-4b65-8cf5-079d17407775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98,\n",
       " array([[70, 66, 74, 79, 84],\n",
       "        [71, 56, 52,  4, 91],\n",
       "        [45, 21, 70, 59, 63],\n",
       "        [52, 66, 81, 47, 96],\n",
       "        [75, 98, 56, 59, 77]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(X)), X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ad82b4-0ed9-48e1-8053-8f06bbf0c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_lasso_model = quic_graph_lasso.QuicGraphicalLasso(init_method='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ef34a2-2e4d-4f69-b064-3e7993731a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = graph_lasso_model.fit(cov, init_method='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafcf15b-0074-4e91-8e83-07a5c26ae2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1725.7 , -281.9 , -671.95, -349.65, -176.85],\n",
       "       [-281.9 ,  358.8 ,  160.9 ,  261.8 , -156.3 ],\n",
       "       [-671.95,  160.9 , 1006.7 ,  397.65, -884.4 ],\n",
       "       [-349.65,  261.8 ,  397.65,  735.3 , -121.3 ],\n",
       "       [-176.85, -156.3 , -884.4 , -121.3 , 1419.8 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50d574b-ab0a-4dc5-ba45-c16203dac547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lam': 0.5,\n",
       " 'mode': 'default',\n",
       " 'tol': 1e-06,\n",
       " 'max_iter': 1000,\n",
       " 'Theta0': None,\n",
       " 'Sigma0': None,\n",
       " 'method': 'quic',\n",
       " 'verbose': 0,\n",
       " 'path': None,\n",
       " 'score_metric': 'log_likelihood',\n",
       " 'init_method': 'precomputed',\n",
       " 'auto_scale': True,\n",
       " 'opt_': 39.108811017020386,\n",
       " 'cputime_': 9.6e-05,\n",
       " 'iters_': 20,\n",
       " 'duality_gap_': -6.938893903907228e-17,\n",
       " 'sample_covariance_': array([[1725.7 , -281.9 , -671.95, -349.65, -176.85],\n",
       "        [-281.9 ,  358.8 ,  160.9 ,  261.8 , -156.3 ],\n",
       "        [-671.95,  160.9 , 1006.7 ,  397.65, -884.4 ],\n",
       "        [-349.65,  261.8 ,  397.65,  735.3 , -121.3 ],\n",
       "        [-176.85, -156.3 , -884.4 , -121.3 , 1419.8 ]]),\n",
       " 'lam_scale_': 1725.7,\n",
       " 'is_fitted_': True,\n",
       " 'path_': None,\n",
       " 'n_samples_': 5,\n",
       " 'n_features_': 5,\n",
       " 'precision_': array([[5.79474996e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 2.78706800e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 9.93667447e-04, 0.00000000e+00,\n",
       "         1.50820774e-05],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.35998912e-03,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.50820774e-05, 0.00000000e+00,\n",
       "         7.04553471e-04]]),\n",
       " 'covariance_': array([[1725.7 ,    0.  ,    0.  ,    0.  ,   -0.  ],\n",
       "        [   0.  ,  358.8 ,    0.  ,    0.  ,   -0.  ],\n",
       "        [   0.  ,    0.  , 1006.7 ,    0.  ,  -21.55],\n",
       "        [   0.  ,    0.  ,    0.  ,  735.3 ,   -0.  ],\n",
       "        [  -0.  ,   -0.  ,  -21.55,   -0.  , 1419.8 ]]),\n",
       " 'n_features_in_': 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ad26494-3c53-4979-996c-aedecd48098c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 696.3 , -479.95, -618.2 ,   72.35,  184.95],\n",
       "       [-479.95,  875.3 ,  295.55,  301.6 ,  714.2 ],\n",
       "       [-618.2 ,  295.55,  800.3 , -127.9 , -221.55],\n",
       "       [  72.35,  301.6 , -127.9 ,  570.7 ,  415.4 ],\n",
       "       [ 184.95,  714.2 , -221.55,  415.4 , 1522.3 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fd092c8-3207-42b3-bc49-dcdce1d38cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 696.3 , -479.95, -618.2 ,   72.35,  184.95],\n",
       "       [-479.95,  875.3 ,  295.55,  301.6 ,  714.2 ],\n",
       "       [-618.2 ,  295.55,  800.3 , -127.9 , -221.55],\n",
       "       [  72.35,  301.6 , -127.9 ,  570.7 ,  415.4 ],\n",
       "       [ 184.95,  714.2 , -221.55,  415.4 , 1522.3 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sample_covariance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f924e2-1fd2-4fa0-ba88-eb29debeea7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function corrcoef in module numpy:\n",
      "\n",
      "corrcoef(x, y=None, rowvar=True, bias=<no value>, ddof=<no value>, *, dtype=None)\n",
      "    Return Pearson product-moment correlation coefficients.\n",
      "    \n",
      "    Please refer to the documentation for `cov` for more detail.  The\n",
      "    relationship between the correlation coefficient matrix, `R`, and the\n",
      "    covariance matrix, `C`, is\n",
      "    \n",
      "    .. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }\n",
      "    \n",
      "    The values of `R` are between -1 and 1, inclusive.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        A 1-D or 2-D array containing multiple variables and observations.\n",
      "        Each row of `x` represents a variable, and each column a single\n",
      "        observation of all those variables. Also see `rowvar` below.\n",
      "    y : array_like, optional\n",
      "        An additional set of variables and observations. `y` has the same\n",
      "        shape as `x`.\n",
      "    rowvar : bool, optional\n",
      "        If `rowvar` is True (default), then each row represents a\n",
      "        variable, with observations in the columns. Otherwise, the relationship\n",
      "        is transposed: each column represents a variable, while the rows\n",
      "        contain observations.\n",
      "    bias : _NoValue, optional\n",
      "        Has no effect, do not use.\n",
      "    \n",
      "        .. deprecated:: 1.10.0\n",
      "    ddof : _NoValue, optional\n",
      "        Has no effect, do not use.\n",
      "    \n",
      "        .. deprecated:: 1.10.0\n",
      "    dtype : data-type, optional\n",
      "        Data-type of the result. By default, the return data-type will have\n",
      "        at least `numpy.float64` precision.\n",
      "    \n",
      "        .. versionadded:: 1.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    R : ndarray\n",
      "        The correlation coefficient matrix of the variables.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    cov : Covariance matrix\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Due to floating point rounding the resulting array may not be Hermitian,\n",
      "    the diagonal elements may not be 1, and the elements may not satisfy the\n",
      "    inequality abs(a) <= 1. The real and imaginary parts are clipped to the\n",
      "    interval [-1,  1] in an attempt to improve on that situation but is not\n",
      "    much help in the complex case.\n",
      "    \n",
      "    This function accepts but discards arguments `bias` and `ddof`.  This is\n",
      "    for backwards compatibility with previous versions of this function.  These\n",
      "    arguments had no effect on the return values of the function and can be\n",
      "    safely ignored in this and previous versions of numpy.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    In this example we generate two random arrays, ``xarr`` and ``yarr``, and\n",
      "    compute the row-wise and column-wise Pearson correlation coefficients,\n",
      "    ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise\n",
      "    Pearson correlation coefficients between the variables of ``xarr``.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> rng = np.random.default_rng(seed=42)\n",
      "    >>> xarr = rng.random((3, 3))\n",
      "    >>> xarr\n",
      "    array([[0.77395605, 0.43887844, 0.85859792],\n",
      "           [0.69736803, 0.09417735, 0.97562235],\n",
      "           [0.7611397 , 0.78606431, 0.12811363]])\n",
      "    >>> R1 = np.corrcoef(xarr)\n",
      "    >>> R1\n",
      "    array([[ 1.        ,  0.99256089, -0.68080986],\n",
      "           [ 0.99256089,  1.        , -0.76492172],\n",
      "           [-0.68080986, -0.76492172,  1.        ]])\n",
      "    \n",
      "    If we add another set of variables and observations ``yarr``, we can\n",
      "    compute the row-wise Pearson correlation coefficients between the\n",
      "    variables in ``xarr`` and ``yarr``.\n",
      "    \n",
      "    >>> yarr = rng.random((3, 3))\n",
      "    >>> yarr\n",
      "    array([[0.45038594, 0.37079802, 0.92676499],\n",
      "           [0.64386512, 0.82276161, 0.4434142 ],\n",
      "           [0.22723872, 0.55458479, 0.06381726]])\n",
      "    >>> R2 = np.corrcoef(xarr, yarr)\n",
      "    >>> R2\n",
      "    array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,\n",
      "            -0.99004057],\n",
      "           [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,\n",
      "            -0.99981569],\n",
      "           [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,\n",
      "             0.77714685],\n",
      "           [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,\n",
      "            -0.83571711],\n",
      "           [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,\n",
      "             0.97517215],\n",
      "           [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,\n",
      "             1.        ]])\n",
      "    \n",
      "    Finally if we use the option ``rowvar=False``, the columns are now\n",
      "    being treated as the variables and we will find the column-wise Pearson\n",
      "    correlation coefficients between variables in ``xarr`` and ``yarr``.\n",
      "    \n",
      "    >>> R3 = np.corrcoef(xarr, yarr, rowvar=False)\n",
      "    >>> R3\n",
      "    array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,\n",
      "             0.22423734],\n",
      "           [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,\n",
      "            -0.44069024],\n",
      "           [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,\n",
      "             0.75137473],\n",
      "           [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,\n",
      "             0.47536961],\n",
      "           [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,\n",
      "            -0.46666491],\n",
      "           [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,\n",
      "             1.        ]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30215a5c-36d2-4315-9d94-6af60a6dbb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuicGraphicalLasso in module quic_graph_lasso object:\n",
      "\n",
      "class QuicGraphicalLasso(inverse_covariance.InverseCovarianceEstimator)\n",
      " |  QuicGraphicalLasso(lam=0.5, mode='default', tol=1e-06, max_iter=1000, Theta0=None, Sigma0=None, path=None, method='quic', verbose=0, score_metric='log_likelihood', init_method='corrcoef', auto_scale=True)\n",
      " |  \n",
      " |  Computes a sparse inverse covariance matrix estimation using quadratic\n",
      " |  approximation.\n",
      " |  \n",
      " |  The inverse covariance is estimated the sample covariance estimate\n",
      " |  $S$ as an input such that:\n",
      " |  \n",
      " |  $T_hat = max_{\\Theta} logdet(Theta) - Trace(ThetaS) - \\lambda|\\Theta|_1 $\n",
      " |  \n",
      " |  Parameters\n",
      " |  -----------\n",
      " |  lam : scalar or 2D ndarray, shape (n_features, n_features) (default=0.5)\n",
      " |      Regularization parameters per element of the inverse covariance matrix.\n",
      " |  \n",
      " |      If a scalar lambda is used, a penalty matrix will be generated\n",
      " |      containing lambda for all values in both upper and lower triangles\n",
      " |      and zeros along the diagonal.  This differs from the scalar graphical\n",
      " |      lasso by the diagonal. To replicate the scalar formulation you must\n",
      " |      manualy pass in lam * np.ones((n_features, n_features)).\n",
      " |  \n",
      " |  mode : one of 'default', 'path', or 'trace'\n",
      " |      Computation mode.\n",
      " |  \n",
      " |  tol : float (default=1e-6)\n",
      " |      Convergence threshold.\n",
      " |  \n",
      " |  max_iter : int (default=1000)\n",
      " |      Maximum number of Newton iterations.\n",
      " |  \n",
      " |  Theta0 : 2D ndarray, shape (n_features, n_features) (default=None)\n",
      " |      Initial guess for the inverse covariance matrix. If not provided, the\n",
      " |      diagonal identity matrix is used.\n",
      " |  \n",
      " |  Sigma0 : 2D ndarray, shape (n_features, n_features) (default=None)\n",
      " |      Initial guess for the covariance matrix. If not provided the diagonal\n",
      " |      identity matrix is used.\n",
      " |  \n",
      " |  path : array of floats (default=None)\n",
      " |      In \"path\" mode, an array of float values for scaling lam.\n",
      " |      The path must be sorted largest to smallest.  This class will auto sort\n",
      " |      this, in which case indices correspond to self.path_\n",
      " |  \n",
      " |  method : 'quic' or 'bigquic', ... (default=quic)\n",
      " |      Currently only 'quic' is supported.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Used in quic routine.\n",
      " |  \n",
      " |  score_metric : one of 'log_likelihood' (default), 'frobenius', 'spectral',\n",
      " |                'kl', or 'quadratic'\n",
      " |      Used for computing self.score().\n",
      " |  \n",
      " |  init_method : one of 'corrcoef', 'cov', 'spearman', 'kendalltau',\n",
      " |      or a custom function.\n",
      " |      Computes initial covariance and scales lambda appropriately.\n",
      " |      Using the custom function extends graphical model estimation to\n",
      " |      distributions beyond the multivariate Gaussian.\n",
      " |      The `spearman` or `kendalltau` options extend inverse covariance\n",
      " |      estimation to nonparanormal and transelliptic graphical models.\n",
      " |      Custom function must return ((n_features, n_features) ndarray, float)\n",
      " |      where the scalar parameter will be used to scale the penalty lam.\n",
      " |  \n",
      " |  auto_scale : bool\n",
      " |      If True, will compute self.lam_scale_ = max off-diagonal value when\n",
      " |      init_method='cov'.\n",
      " |      If false, then self.lam_scale_ = 1.\n",
      " |      lam_scale_ is used to scale user-supplied self.lam during fit.\n",
      " |  \n",
      " |  Methods\n",
      " |  ----------\n",
      " |  lam_at_index(lidx) :  Compute the scaled lambda used at index lidx.\n",
      " |      The parameter lidx is ignored when mode='default'.  Can use self.lam_\n",
      " |      for convenience in this case.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  covariance_ : 2D ndarray, shape (n_features, n_features)\n",
      " |      Estimated covariance matrix\n",
      " |      If mode='path', this is 2D ndarray, shape (len(path), n_features ** 2)\n",
      " |  \n",
      " |  precision_ : 2D ndarray, shape (n_features, n_features)\n",
      " |      Estimated pseudo-inverse matrix.\n",
      " |      If mode='path', this is 2D ndarray, shape (len(path), n_features ** 2)\n",
      " |  \n",
      " |  sample_covariance_ : 2D ndarray, shape (n_features, n_features)\n",
      " |      Estimated sample covariance matrix\n",
      " |  \n",
      " |  lam_ : (float) or 2D ndarray, shape (n_features, n_features)\n",
      " |      When mode='default', this is the lambda used in fit (lam * lam_scale_)\n",
      " |  \n",
      " |  lam_scale_ : (float)\n",
      " |      Additional scaling factor on lambda (due to magnitude of\n",
      " |      sample_covariance_ values).\n",
      " |  \n",
      " |  path_ : None or array of floats\n",
      " |      Sorted (largest to smallest) path.  This will be None if not in path\n",
      " |      mode.\n",
      " |  \n",
      " |  opt_ :\n",
      " |  \n",
      " |  cputime_ :\n",
      " |  \n",
      " |  iters_ :\n",
      " |  \n",
      " |  duality_gap_ :\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      QuicGraphicalLasso\n",
      " |      inverse_covariance.InverseCovarianceEstimator\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, lam=0.5, mode='default', tol=1e-06, max_iter=1000, Theta0=None, Sigma0=None, path=None, method='quic', verbose=0, score_metric='log_likelihood', init_method='corrcoef', auto_scale=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, **fit_params)\n",
      " |      Fits the inverse covariance model according to the given training\n",
      " |      data and parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : 2D ndarray, shape (n_features, n_features)\n",
      " |          Input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  lam_at_index(self, lidx)\n",
      " |      Compute the scaled lambda used at index lidx.\n",
      " |  \n",
      " |  set_score_request(self: quic_graph_lasso.QuicGraphicalLasso, *, X_test: Union[bool, NoneType, str] = '$UNCHANGED$') -> quic_graph_lasso.QuicGraphicalLasso\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X_test : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``X_test`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lam_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from inverse_covariance.InverseCovarianceEstimator:\n",
      " |  \n",
      " |  cov_error(self, comp_cov, score_metric='frobenius')\n",
      " |      Computes the covariance error vs. comp_cov.\n",
      " |      \n",
      " |      May require self.path_\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      comp_cov : array-like, shape = (n_features, n_features)\n",
      " |          The precision to compare with.\n",
      " |          This should normally be the test sample covariance/precision.\n",
      " |      \n",
      " |      scaling : bool\n",
      " |          If True, the squared error norm is divided by n_features.\n",
      " |          If False (default), the squared error norm is not rescaled.\n",
      " |      \n",
      " |      score_metric : str\n",
      " |          The type of norm used to compute the error between the estimated\n",
      " |          self.precision, self.covariance and the reference `comp_cov`.\n",
      " |          Available error types:\n",
      " |      \n",
      " |          - 'frobenius' (default): sqrt(tr(A^t.A))\n",
      " |          - 'spectral': sqrt(max(eigenvalues(A^t.A))\n",
      " |          - 'kl': kl-divergence\n",
      " |          - 'quadratic': quadratic loss\n",
      " |          - 'log_likelihood': negative log likelihood\n",
      " |      \n",
      " |      squared : bool\n",
      " |          Whether to compute the squared error norm or the error norm.\n",
      " |          If True (default), the squared error norm is returned.\n",
      " |          If False, the error norm is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The min error between `self.covariance_` and `comp_cov`.\n",
      " |      \n",
      " |      If self.precision_ is a list, returns errors for each matrix, otherwise\n",
      " |      returns a scalar.\n",
      " |  \n",
      " |  ebic(self, gamma=0)\n",
      " |      Compute EBIC scores for each model. If model is not \"path\" then\n",
      " |      returns a scalar score value.\n",
      " |      \n",
      " |      May require self.path_\n",
      " |      \n",
      " |      See:\n",
      " |      Extended Bayesian Information Criteria for Gaussian Graphical Models\n",
      " |      R. Foygel and M. Drton\n",
      " |      NIPS 2010\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gamma : (float) \\in (0, 1)\n",
      " |          Choice of gamma=0 leads to classical BIC\n",
      " |          Positive gamma leads to stronger penalization of large graphs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Scalar ebic score or list of ebic scores.\n",
      " |  \n",
      " |  ebic_select(self, gamma=0)\n",
      " |      Uses Extended Bayesian Information Criteria for model selection.\n",
      " |      \n",
      " |      Can only be used in path mode (doesn't really make sense otherwise).\n",
      " |      \n",
      " |      See:\n",
      " |      Extended Bayesian Information Criteria for Gaussian Graphical Models\n",
      " |      R. Foygel and M. Drton\n",
      " |      NIPS 2010\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gamma : (float) \\in (0, 1)\n",
      " |          Choice of gamma=0 leads to classical BIC\n",
      " |          Positive gamma leads to stronger penalization of large graphs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Lambda index with best ebic score.  When multiple ebic scores are the\n",
      " |      same, returns the smallest lambda (largest index) with minimum score.\n",
      " |  \n",
      " |  init_coefs(self, X)\n",
      " |      Computes ...\n",
      " |      \n",
      " |      Initialize the following values:\n",
      " |          self.n_samples\n",
      " |          self.n_features\n",
      " |          self.sample_covariance_\n",
      " |          self.lam_scale_\n",
      " |  \n",
      " |  score(self, X_test, y=None)\n",
      " |      Computes the score between cov/prec of sample covariance of X_test\n",
      " |      and X via 'score_metric'.\n",
      " |      \n",
      " |      Note: We want to maximize score so we return the negative error.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X_test : array-like, shape = [n_samples, n_features]\n",
      " |          Test data of which we compute the likelihood, where n_samples is\n",
      " |          the number of samples and n_features is the number of features.\n",
      " |          X_test is assumed to be drawn from the same distribution than\n",
      " |          the data used in fit (including centering).\n",
      " |      \n",
      " |      y : not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : float or list of floats\n",
      " |          The negative of the min error between `self.covariance_` and\n",
      " |          the sample covariance of X_test.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(graph_lasso_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cd603-be5a-4be1-aba6-301159b3e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20201669-d122-42b8-81fb-3b2a5567474c",
   "metadata": {},
   "source": [
    "## Tests on the fake sc_atac data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82dd4a6-99a6-4a93-be1f-f7c01598bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# Create fake single-cell atac-seq data\n",
    "counts = pd.DataFrame(np.random.randint(0, 100, size=(50, 200)),\n",
    "                      index=['Cell_'+i for i in map(str, range(50))],\n",
    "                      columns=['chr1_'+i+'_'+i for i in map(str, range(200))])\n",
    "\n",
    "atac = ad.AnnData(counts)\n",
    "\n",
    "\n",
    "def add_region_infos(AnnData,\n",
    "                     sep=('_', '_'),\n",
    "                     inplace=True):\n",
    "    \"\"\"\n",
    "    Get region informations from the var_names of AnnData object.\n",
    "    e.g. chr1_12345_12346 -> 'chromosome' : chr1, 'start' : 12345, 'end' : 12346\n",
    "    These info will be added to var of AnnData object.\n",
    "        adata.var['chromosome'] : chromosome\n",
    "        adata.var['start'] : start position\n",
    "        adata.var['end'] : end position\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AnnData : AnnData object\n",
    "        AnnData object with var_names as region names.\n",
    "    sep : tuple, optional\n",
    "        Separator of region names. The default is ('_', '_').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    AnnData : AnnData object\n",
    "        AnnData object with region informations in var.\n",
    "    \"\"\"\n",
    "    # Check if user wants to modify AnnData inplace or return a copy\n",
    "    if inplace:\n",
    "        pass\n",
    "    else:\n",
    "        AnnData = AnnData.copy()\n",
    "    regions_list = AnnData.var_names\n",
    "\n",
    "    # Replace sep[1] with sep[0] to make it easier to split\n",
    "    regions_list = regions_list.str.replace(sep[1], sep[0])\n",
    "\n",
    "    # Split region names\n",
    "    regions_list = regions_list.str.split(sep[0]).tolist()\n",
    "\n",
    "    # Check if all regions have the same number of elements\n",
    "    if set([len(i) for i in regions_list]) != set([3]):\n",
    "        raise ValueError(\"\"\"Not all regions have the same number of elements.\n",
    "                         Check if sep is correct, it should be ({}, {}),\n",
    "                         with only one occurence each in region names.\"\"\".format(sep[0], sep[1]))\n",
    "\n",
    "    # Extract region informations from var_names\n",
    "    region_infos = pd.DataFrame(regions_list,\n",
    "                                index=AnnData.var_names,\n",
    "                                columns=['chromosome', 'start', 'end'])\n",
    "\n",
    "    # Convert start and end to int\n",
    "    region_infos['start'] = region_infos['start'].astype(int)\n",
    "    region_infos['end'] = region_infos['end'].astype(int)\n",
    "\n",
    "    # Add region informations to var\n",
    "    AnnData.var['chromosome'] = region_infos['chromosome']\n",
    "    AnnData.var['start'] = region_infos['start']\n",
    "    AnnData.var['end'] = region_infos['end']\n",
    "\n",
    "    sort_regions(AnnData)\n",
    "    # Return AnnData if inplace is False\n",
    "    if inplace:\n",
    "        pass\n",
    "    else:\n",
    "        return AnnData\n",
    "\n",
    "\n",
    "def sort_regions(AnnData):\n",
    "    \"\"\"\n",
    "    Sort regions by chromosome and start position.\n",
    "    \"\"\"\n",
    "    AnnData.var.sort_values(['chromosome', 'start'], inplace=True)\n",
    "    return AnnData\n",
    "\n",
    "\n",
    "def get_distance_regions(AnnData, chromosomes=None):\n",
    "    \"\"\"\n",
    "    Get distance between regions.\n",
    "    \"\"\"\n",
    "    # Check if chromosomes is None\n",
    "    if chromosomes is None:\n",
    "        # Get chromosome list\n",
    "        chromosomes = AnnData.var['chromosome'].unique().tolist()\n",
    "    else:\n",
    "        if not np.array([i in AnnData.var['chromosome'].unique() for i in chromosomes]).all():\n",
    "            raise ValueError(\"\"\"Chromosomes should be in AnnData.var['chromosome'].\n",
    "                                Check if chromosomes is correct.\"\"\")\n",
    "\n",
    "    # A dictionary to store distance between regions for each chromosome\n",
    "    distances = {}\n",
    "\n",
    "    # Get distance between regions for each chromosome\n",
    "    for chromosome in chromosomes:\n",
    "        chr_mask = AnnData.var['chromosome']==chromosome\n",
    "        # Store start and end positions in two arrays\n",
    "        m, n = np.meshgrid(AnnData.var['start'].values[chr_mask],\n",
    "                           AnnData.var['end'].values[chr_mask])\n",
    "\n",
    "        # Get distance between start of region m and end of region n\n",
    "        distance = np.abs(m-n)\n",
    "        # Substract length of the region to get distance between\n",
    "        # end of region m and start of region n\n",
    "        # a.k.a. distance between closest bases of two regions\n",
    "        distance = (distance.T-(AnnData.var['end'].values[chr_mask]\n",
    "                                - AnnData.var['start'].values[chr_mask])).T\n",
    "\n",
    "        # Remove diagonal (distance between a region and itself)\n",
    "        distance -= np.diag(distance)\n",
    "\n",
    "        # Keep upper triangle of the distance matrix\n",
    "        # (we don't want to calculate the same connection twice)\n",
    "        distance = np.triu(distance, k=1)\n",
    "\n",
    "        # Test if distance is negative\n",
    "        if np.any(distance < 0):\n",
    "            raise ValueError(\"\"\"Distance between regions should be positive.\n",
    "                            You might have overlapping regions.\"\"\")\n",
    "\n",
    "        # Store distance in a dictionary\n",
    "        distances[chromosome] = distance\n",
    "\n",
    "    # Return distance\n",
    "    return distances\n",
    "\n",
    "\n",
    "def potential_connections(AnnData, threshold, chromosomes=None):\n",
    "    \"\"\"\n",
    "    Get potential connections between regions based on distance.\n",
    "    \"\"\"\n",
    "    # Check if chromosomes is None\n",
    "    if chromosomes is None:\n",
    "        # Get chromosome list\n",
    "        chromosomes = AnnData.var['chromosome'].unique().tolist()\n",
    "    else:\n",
    "        if not np.array([i in AnnData.var['chromosome'].unique() for i in chromosomes]).all():\n",
    "            raise ValueError(\"\"\"Chromosomes should be in AnnData.var['chromosome'].\n",
    "                                Check if chromosomes is correct.\"\"\")\n",
    "    # Get distance between regions\n",
    "    distances = get_distance_regions(AnnData, chromosomes=chromosomes)\n",
    "\n",
    "    potential_connections = {}\n",
    "    # Get potential connections\n",
    "    for chromosome in chromosomes:\n",
    "        print(\"Getting potential connections for chromosome {}...\".format(chromosome))\n",
    "        # Get potential connections for each chromosome\n",
    "        distance = distances[chromosome]\n",
    "        potential_chr_co = np.where((distance <= threshold)\n",
    "                                         & (distance > 0))\n",
    "\n",
    "        # Store potential connections in a dictionary\n",
    "        potential_connections[chromosome] = potential_chr_co\n",
    "\n",
    "    # Return potential connections\n",
    "    return potential_connections\n",
    "\n",
    "\n",
    "def corrcoef_connections(AnnData, potential_connections, as_sparse=True):\n",
    "    \"\"\"\n",
    "    Get correlation coefficient between regions.\n",
    "    \"\"\"\n",
    "    # Transform potential_connections into a sparse matrix\n",
    "    potential_connections = global_sparse(AnnData, potential_connections)\n",
    "\n",
    "    # Get correlation coefficient between regions\n",
    "    corr_coefs = [np.cov(AnnData.X[:, potential_connections.row[i]],\n",
    "                              AnnData.X[:, potential_connections.col[i]])[0, 1]\n",
    "                  for i in tqdm.tqdm(range(len(potential_connections.row)))]\n",
    "\n",
    "    # Convert to sparse matrix if as_sparse is True\n",
    "    if as_sparse:\n",
    "        corr_coefs = sp.sparse.coo_matrix((corr_coefs,\n",
    "                                           (potential_connections.row,\n",
    "                                            potential_connections.col)),\n",
    "                                          shape=(AnnData.shape[1],\n",
    "                                                 AnnData.shape[1]))\n",
    "    else:\n",
    "        corr_coefs = np.array(corr_coefs)\n",
    "    \n",
    "    # Return correlation coefficients\n",
    "    return corr_coefs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # A dictionary storing the informations to create a sparse matrix of potential connections\n",
    "    corr_coefs = {}\n",
    "    corr_coefs['values'] = np.array([])\n",
    "    corr_coefs['idx'] = np.array([])\n",
    "    corr_coefs['idy'] = np.array([])\n",
    "\n",
    "    # Get correlation coefficient between regions for each chromosome\n",
    "    for chromosome in potential_connections.keys():\n",
    "        chr_mask = AnnData.var['chromosome']==chromosome\n",
    "        print(\"Getting correlation coefficient for chromosome {}...\".format(chromosome))\n",
    "\n",
    "        corr_coefs_chr = [np.corrcoef(AnnData.X[:,chr_mask][:, potential_connections[chromosome][0][i]],\n",
    "                                      AnnData.X[:,chr_mask][:, potential_connections[chromosome][1][i]])[0, 1]\n",
    "                          for i in range(len(potential_connections[chromosome][0]))]\n",
    "\n",
    "        # Store correlation coefficient in a dictionary\n",
    "        corr_coefs[chromosome] = corr_coefs_chr\n",
    "    return corr_coefs\n",
    "\n",
    "\n",
    "def global_sparse(AnnData, chr_idx, values = 1):\n",
    "    \"\"\"\n",
    "    Create a sparse matrix from a dictionary of np.where output on 'regions*regions' matrices.of different chromosomes.\n",
    "    using global indices of an AnnData object.\n",
    "\n",
    "    e.g.:\n",
    "    AnnData.var_names = ['chr1_12345_12346', 'chr1_12347_12348', 'chr2_12345_12346', 'chr2_12347_12348']\n",
    "    And we know values for  (chr1_12345_12346, chr1_12347_12348),\n",
    "                            (chr1_12347_12348, chr1_12345_12346),\n",
    "                            (chr2_12345_12346, chr2_12345_12346),\n",
    "                            (chr2_12347_12348, chr2_12347_12348).\n",
    "    \n",
    "    We can create a sparse matrix with 'chr_dic' defined as:\n",
    "    chr_dic = {'chr1' : np.array([[1, 0], [0, 1]]),\n",
    "               'chr2' : np.array([[0, 0], [1, 1]])}\n",
    "\n",
    "    sparse_mtx = global_sparse(AnnData, chr_dic)\n",
    "    sparse_mtx\n",
    "    'OUTPUT' :               chr_1_12345_12346  chr_1_12347_12348  chr_2_12345_12346  chr_2_12347_12348\n",
    "    chr_1_12345_12346                 *                  1                  *                  *\n",
    "    chr_1_12347_12348                 1                  *                  *                  *\n",
    "    chr_2_12345_12346                 *                  *                  1                  *\n",
    "    chr_2_12347_12348                 *                  *                  *                  1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    AnnData : AnnData object\n",
    "        AnnData object with var_names as region names.\n",
    "    chr_dic : dictionary\n",
    "        Dictionary of matrices (regions*regions) of different chromosomes.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "    data['values'] = np.array([])\n",
    "    data['idx'] = np.array([])\n",
    "    data['idy'] = np.array([])\n",
    "\n",
    "    for chromosome in chr_idx.keys():\n",
    "        chr_mask = AnnData.var['chromosome'] == chromosome\n",
    "        # Get region names (needed to get global indices)\n",
    "        indices_names = AnnData.var_names[chr_mask][chr_idx[chromosome][0]]\n",
    "        columns_names = AnnData.var_names[chr_mask][chr_idx[chromosome][1]]\n",
    "        map_indices = {AnnData.var_names[i]: i\n",
    "                       for i in range(len(AnnData.var_names))}\n",
    "\n",
    "        # Add global indices of potential connections\n",
    "        data['idx'] = np.concatenate([data['idx'],\n",
    "                                      indices_names.map(map_indices).values])\n",
    "        data['idy'] = np.concatenate([data['idy'],\n",
    "                                      columns_names.map(map_indices).values])\n",
    "        # Add values of potential connections\n",
    "        if type(values) == int:\n",
    "            val = np.repeat(values, len(chr_idx[chromosome][0]))\n",
    "        elif type(values) == dict:\n",
    "            val = values[chromosome]\n",
    "        else:\n",
    "            raise ValueError(\"\"\"values should be an int,\n",
    "                             or a dict (of numpy arrays).\"\"\")\n",
    "        data['values'] = np.concatenate([data['values'],\n",
    "                                         val])\n",
    "    # Create sparse matrix\n",
    "    sparse_data = sp.sparse.coo_matrix((data['values'],\n",
    "                                        (data['idx'],\n",
    "                                         data['idy'])),\n",
    "                                       shape=(AnnData.shape[1],\n",
    "                                              AnnData.shape[1]))\n",
    "\n",
    "    return sparse_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235b6153-cd29-410a-ae17-7b806343b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scipy as sp\n",
    "\n",
    "import quic_graph_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ec19b-b523-4bfd-a2d2-6e672de8e92e",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14bc209-6842-481e-bdce-1511164cc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake single-cell atac-seq data\n",
    "nb_cells = 300\n",
    "nb_chr = 10\n",
    "nb_regions_per_chr = 200\n",
    "between_reg = 1000\n",
    "size_reg = 50\n",
    "\n",
    "counts = []\n",
    "for chr in range(nb_chr):\n",
    "    counts.append(pd.DataFrame(np.random.randint(0,100, size=(nb_cells, nb_regions_per_chr)),\n",
    "                        index=['Cell_'+j for j in map(str, range(nb_cells))],\n",
    "                        columns=['chr'+str(chr)+'_'+str(i)+'_'+str(i+size_reg) for i in range(1, nb_regions_per_chr*between_reg+1, between_reg)]))\n",
    "atac = ad.AnnData(pd.concat(counts, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f44cd6bd-370e-4b76-bd56-7549e883e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc9e2-4991-4fec-a046-0c9a4c51b795",
   "metadata": {},
   "source": [
    "### 1/5 Add region position in AnnData.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3e7fdec9-e454-4665-9670-8657a5d0ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_region_infos(atac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38489ac6-243f-4ef0-8b9b-37c75b11f980",
   "metadata": {},
   "source": [
    "### 2/5 Get potential pairs of connected regions (per chromosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "976931ed-4fb0-4c12-bfe1-8e8520b42054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting potential connections for chromosome chr0...\n",
      "Getting potential connections for chromosome chr1...\n",
      "Getting potential connections for chromosome chr2...\n",
      "Getting potential connections for chromosome chr3...\n",
      "Getting potential connections for chromosome chr4...\n",
      "Getting potential connections for chromosome chr5...\n",
      "Getting potential connections for chromosome chr6...\n",
      "Getting potential connections for chromosome chr7...\n",
      "Getting potential connections for chromosome chr8...\n",
      "Getting potential connections for chromosome chr9...\n"
     ]
    }
   ],
   "source": [
    "potential_connections_atac = potential_connections(atac, threshold=distance_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e03ac-c713-4eb4-8c76-c337741ead9f",
   "metadata": {},
   "source": [
    "### 3/5 Calculate correlations (cov in fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "218343ce-e09d-4f33-ae28-03845d446964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87250/87250 [00:01<00:00, 45080.24it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_cov = corrcoef_connections(atac, potential_connections_atac, as_sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa7207-b5fc-40c4-8fa8-42499a866f69",
   "metadata": {},
   "source": [
    "### 4/5 Calculate penalty associated with distance for each pair of regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca0b2b-a6ca-47ea-9b56-b321ff702cac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "_Calculate the euclidean distance between all pair of region (per chromosome)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b8bd7580-e96d-4429-a54e-37f202aacfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = get_distance_regions(atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f8acb8fe-d444-40bd-b85f-fd800c1d73b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.sparse.csr_matrix(sample_cov)[[4,6],:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2846afce-9a14-4f3e-9aa3-f93c349e783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr0': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr1': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr2': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr3': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr4': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr5': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr6': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr7': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr8': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]]),\n",
       " 'chr9': array([[     0,    900,   1900, ..., 196900, 197900, 198900],\n",
       "        [     0,      0,    900, ..., 195900, 196900, 197900],\n",
       "        [     0,      0,      0, ..., 194900, 195900, 196900],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      0,    900,   1900],\n",
       "        [     0,      0,      0, ...,      0,      0,    900],\n",
       "        [     0,      0,      0, ...,      0,      0,      0]])}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sliding_submatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e0f6b-0b92-4030-872a-2e93af131cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5a4c16cf-6727-43cc-958d-9e9976e91cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "graph_lasso_model = quic_graph_lasso.QuicGraphicalLasso(init_method='precomputed', lam=np.random.random(size = (k,k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ad5d7d88-af06-4986-9e3a-a0d70838c70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_cov = sample_cov.toarray() + sample_cov.T.toarray()\n",
    "#sample_cov = sample_cov - (np.diag(sample_cov) - 1e-4) * np.eye(sample_cov.shape[0])\n",
    "sample_cov += 1e-4 * np.eye(sample_cov.shape[0])\n",
    "results = graph_lasso_model.fit(sample_cov[:k,:k], tol=1e-6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05c7c381-f94c-4123-9fe4-4a81f14721a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "sample_cov[:30,:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d990ea53-d6dc-4035-87f8-e0b80ed8c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-648.7245551044573"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results.covariance_ - np.diag(results.covariance_)*np.eye(len(results.covariance_))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ab67f-fd98-4f48-9586-7552aa6db0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e50b7a43-0ef4-4ec5-8fd6-f5df654ff73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = graph_lasso_model.fit(atac.X[:200,:30].T, tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dd0a6e05-d53f-4db4-9747-9009920f820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1513101824912457"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results.covariance_ - np.diag(results.covariance_)*np.eye(len(results.covariance_))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "664255c8-91a5-4155-86c3-11f99ff92d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_lasso_model = quic_graph_lasso.QuicGraphicalLasso(init_method='precomputed', lam=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6a4a9142-5fdb-48e9-b112-e5cc4c70f076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 87250 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013eb27-566d-4575-95b0-5db8b493561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results.covariance_ - np.diag(results.covariance_)*np.eye(len(results.covariance_))).sum()\n",
    "#results.covariance_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fd6ba-ed5b-468f-b39b-d0d11da4ea51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "56a6ecc6-2b2f-431d-b13b-2ecb8c3d2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.corrcoef(atac.X[:200,:30])\n",
    "cov = cov - (np.eye(len(cov))/np.diag(cov) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f9e14a09-a701-4373-acc2-7ca8a3789c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7231138584847492"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68f9391-7c79-419f-9e52-a110fecf66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea8d333f-a63f-4c9d-b147-38dc71c76846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issparse(sample_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51c4a09-28fc-410c-9755-684e25427314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249.13752508361202"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(sample_cov.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fe4b66b5-0b44-4810-b2b4-74d51f7cc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = sp.sparse.coo_matrix(sample_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1d82e10b-c131-4c0b-8aa5-94555a079254",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = sp.sparse.csr_matrix(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e4a8fd06-cd83-4394-8bf4-8490912f1d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0de55356-e4bb-4a89-9c8d-38a1eea254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blop = np.where(sparse.data>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a7e1eed2-0570-44cf-b7e1-539a9525fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "blop2 = np.where(sparse.data>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d9ff8502-b0b5-4763-aa7c-3f6c9943a597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_mask1 = np.where(sparse.data>0)\n",
    "pos_idx1 = sparse.row[pos_mask1]\n",
    "pos_idy1 = sparse.col[pos_mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "07879678-594c-4278-a75f-e4f2ce741706",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sparse\n",
    "s2 = sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5d010953-1589-400a-9098-0d6323480952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e-04,  2.86204013e+00, -7.87468896e+01, ...,\n",
       "        2.99430881e+01,  2.67039576e+01,  1.00000000e-04])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "10791910-b4ce-49f2-9ecb-1f850dd522c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_coord1 = {(x,y) for x,y,d in zip(s1.row, s1.col, s1.data) if d>=0}\n",
    "pos_coord2 = {(x,y) for x,y,d in zip(s2.row, s2.col, s2.data) if d>=0}\n",
    "pos_coord = pd.DataFrame(pos_coord1.intersection(pos_coord2), columns = ['row', 'col'])\n",
    "\n",
    "neg_coord1 = {(x,y) for x,y,d in zip(s1.row, s1.col, s1.data) if d<=0}\n",
    "neg_coord2 = {(x,y) for x,y,d in zip(s2.row, s2.col, s2.data) if d<=0}\n",
    "neg_coord = pd.DataFrame(neg_coord1.intersection(neg_coord2), columns = ['row', 'col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "dcd85375-8ac4-467f-b1c8-4bd0c8622cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "concat() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[370], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_coord\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_coord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: concat() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "pd.concat(neg_coord , pos_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "cdbd778e-82f6-446f-8734-8efbbe5d97df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1426, 1025, 1467, ..., 1625,  229,  934], dtype=int32)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = {}\n",
    "common['row'] = np.array(bb)[:,0]\n",
    "common['col'] = np.array(bb)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c310a117-bed5-4fa8-ac97-c7029196eb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.00000000e-04, 2.86204013e+00, 6.95003344e+00, ...,\n",
       "         2.99430881e+01, 2.67039576e+01, 1.00000000e-04]])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.sparse.csr_matrix(s1)[(s1 + s2) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c6a8457f-7e2f-4e23-abcb-7003c96b3ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s1.multiply(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c7ac8fc3-6d42-472a-b37a-d90276b2a76c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[317], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m c1 \u001b[38;5;129;01min\u001b[39;00m coord2 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m c1 \u001b[38;5;129;01min\u001b[39;00m coord1]\n",
      "Cell \u001b[0;32mIn[317], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m c1 \u001b[38;5;129;01min\u001b[39;00m coord2 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m c1 \u001b[38;5;129;01min\u001b[39;00m coord1]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "[True if c1 in coord2 else False for c1 in tqdm.tqdm(coord1)]\n",
    "[True if c2 in coord1 else False for c2 in tqdm.tqdm(coord2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "222ce68a-d197-409e-aa0b-fa6a0373e39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shape': (2000, 2000),\n",
       " 'maxprint': 50,\n",
       " 'data': array([ 1.00000000e-04,  2.86204013e+00, -7.87468896e+01, ...,\n",
       "         2.99430881e+01,  2.67039576e+01,  1.00000000e-04]),\n",
       " 'indices': array([   0,    1,    2, ..., 1997, 1998, 1999], dtype=int32),\n",
       " 'indptr': array([     0,     51,    103, ..., 176397, 176449, 176500], dtype=int32)}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5b4c2629-1b99-443d-b128-2f73339c7fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coo_matrix' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[283], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msparse\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sparse[:4,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf79f7d4-d450-4b7f-82ef-fe5676a48c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#%%timeit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_cov\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_cov\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_cov\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:337\u001b[0m, in \u001b[0;36mspmatrix.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an array with more than one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "np.where(np.abs(sp.sparse.csr_matrix(sample_cov)) - np.abs(sp.sparse.csr_matrix(sample_cov) - sp.sparse.csr_matrix(sample_cov)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6af5d2b8-a1ad-4e8d-91f3-2389ef0e7954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0, ..., 1999, 1999, 1999]),\n",
       " array([   0,    1,    2, ..., 1997, 1998, 1999]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9eb42-735f-497a-9d58-dafbfb1057e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
